{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data handeling\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud, STOPWORDS \n",
    "\n",
    "# tweepy stuff\n",
    "import tweepy\n",
    "from AppCred import BEARER_TOKEN, CONSUMER_KEY, CONSUMER_SECRET, ACCESS_TOKEN, ACCESS_TOKEN_SECRET\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from os.path import exists\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the values accordingly\n",
    "consumer_key = CONSUMER_KEY\n",
    "consumer_secret = CONSUMER_SECRET\n",
    "access_token = ACCESS_TOKEN\n",
    "access_token_secret = ACCESS_TOKEN_SECRET\n",
    "  \n",
    "# authorization of consumer key and consumer secret\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "  \n",
    "# set access to user's access key and access secret \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "  \n",
    "# calling the api \n",
    "api = tweepy.API(auth, wait_on_rate_limit= True)\n",
    "\n",
    "# calling the client\n",
    "client = tweepy.Client(bearer_token = BEARER_TOKEN,\n",
    "                        consumer_key = CONSUMER_KEY,\n",
    "                        consumer_secret = CONSUMER_SECRET,\n",
    "                        access_token = ACCESS_TOKEN,\n",
    "                        access_token_secret = ACCESS_TOKEN_SECRET,\n",
    "                        #return_type=dict, \n",
    "                        wait_on_rate_limit=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paginate(iterable, page_size):\n",
    "    while True:\n",
    "        i1, i2 = itertools.tee(iterable)\n",
    "        iterable, page = (itertools.islice(i1, page_size, None),\n",
    "                list(itertools.islice(i2, page_size)))\n",
    "        if len(page) == 0:\n",
    "            break\n",
    "        yield page\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading in the data set\n",
    "d_people = pd.read_excel('immersion.xlsx', sheet_name='People', index_col=0)\n",
    "\n",
    "d_departments = pd.read_excel('immersion.xlsx', sheet_name='Institutes', index_col=0)\n",
    "\n",
    "screen_names = list(d_people['Twitter_handle']) + list(d_departments['Twitter_handle'])\n",
    "\n",
    "print(sum([i != '-' for i in screen_names]))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('d_following.csv'):\n",
    "    d_following = pd.read_csv('d_following.csv', index_col=0)\n",
    "else:\n",
    "    d_following = pd.DataFrame({'account': [], 'following' : []})\n",
    "    d_following.to_csv('d_following.csv')\n",
    "\n",
    "restricted_following = []\n",
    "\n",
    "for i, screen_name in enumerate(screen_names):\n",
    "    if screen_name != '-':\n",
    "        if screen_name not in list(d_following.account):\n",
    "\n",
    "            name = screen_name.split(\"@\")[1]\n",
    "            print(f'Scraping information from {name}: {i} of {len(screen_names)}')\n",
    "\n",
    "            try:\n",
    "                following_ids = api.get_friend_ids(screen_name=name, count = 5000)\n",
    "\n",
    "                following_names = []\n",
    "\n",
    "                for page in paginate(following_ids, 100):\n",
    "                    try:\n",
    "                        results = api.lookup_users(user_id=page)\n",
    "                        for result in results:\n",
    "                            following_names.append(result.screen_name)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                account = [screen_name] * len(following_names)\n",
    "\n",
    "                d_temp = pd.DataFrame({'account': account, 'following' : following_names})\n",
    "\n",
    "                d_following = pd.concat([d_following, d_temp],axis=0)\n",
    "\n",
    "                d_following.to_csv('d_following.csv')\n",
    "                print('saved')\n",
    "\n",
    "            except:\n",
    "                restricted_following.append(screen_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Special treatment for club 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('d_following5000.csv'):\n",
    "    d_following5000 = pd.read_csv('d_following5000.csv', index_col=0)\n",
    "else:\n",
    "    d_following5000 = pd.DataFrame({'account': [], 'following' : []})\n",
    "    d_following5000.to_csv('d_following5000.csv')\n",
    "\n",
    "restricted_followers = []\n",
    "# [' @rebadlernissen', ' @tiagopeixoto', ' @Jan_Vogler']\n",
    "for i, screen_name in enumerate([' @rebadlernissen', ' @tiagopeixoto', ' @Jan_Vogler']):\n",
    "    if screen_name != '-':\n",
    "        if screen_name not in list(d_following5000.account):\n",
    "            print(f'Scraping information from {screen_name.split(\"@\")[1]}: {i+1} of {3}')\n",
    "            ids = []\n",
    "\n",
    "            for fid in tweepy.Cursor(api.get_friend_ids, screen_name=screen_name.split(\"@\")[1], count=100).pages():\n",
    "                ids.append(fid)\n",
    "\n",
    "            l_followers = []\n",
    "            for id in ids:\n",
    "                    try:\n",
    "                        results = api.lookup_users(user_id=id)\n",
    "                        for result in results:\n",
    "                            following_names.append(result.screen_name)\n",
    "                    except:\n",
    "                        pass\n",
    "            account = [screen_name] * len(l_followers)\n",
    "            d_temp = pd.DataFrame({'account': account, 'following' : l_followers})\n",
    "            d_following5000 = pd.concat([d_following5000, d_temp],axis=0)\n",
    "            d_following5000.to_csv('d_following5000.csv')\n",
    "            print('saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c5000 = [' @rebadlernissen', ' @tiagopeixoto', ' @Jan_Vogler']\n",
    "\n",
    "pd.concat([d_following[~d_following.account.isin(c5000)], d_following5000],axis=0).to_csv('d_following_all.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if exists('d_followers.csv'):\n",
    "    d_followers = pd.read_csv('d_followers.csv', index_col=0)\n",
    "else:\n",
    "    d_followers = pd.DataFrame({'account': [], 'followers' : []})\n",
    "    d_followers.to_csv('d_followers.csv')\n",
    "\n",
    "restricted_followers = []\n",
    "\n",
    "for i, screen_name in enumerate(screen_names):\n",
    "    if screen_name != '-':\n",
    "        if screen_name not in list(d_followers.account):\n",
    "\n",
    "            name = screen_name.split(\"@\")[1]\n",
    "            print(f'Scraping information from {name}: {i} of {len(screen_names)}')\n",
    "\n",
    "            try:\n",
    "                follower_ids = api.get_follower_ids(screen_name=name, count = 5000)\n",
    "\n",
    "                follower_names = []\n",
    "\n",
    "                for page in paginate(follower_ids, 100):\n",
    "                    try:\n",
    "                        results = api.lookup_users(user_id=page)\n",
    "                        for result in results:\n",
    "                            follower_names.append(result.screen_name)\n",
    "                    except:\n",
    "                        pass\n",
    "\n",
    "                account = [screen_name] * len(follower_names)\n",
    "\n",
    "                d_temp = pd.DataFrame({'account': account, 'followers' : follower_names})\n",
    "\n",
    "                d_followers = pd.concat([d_followers, d_temp],axis=0)\n",
    "\n",
    "                d_followers.to_csv('d_followers.csv')\n",
    "                print('saved')\n",
    "                #time.sleep(60*2.5)\n",
    "\n",
    "            except:\n",
    "                restricted_followers.append(screen_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_followers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restricted_followers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tweets = pd.DataFrame()\n",
    "restricted_tweets = []\n",
    "\n",
    "for i, screen_name in enumerate(screen_names):\n",
    "    #time.sleep(60*2.5)\n",
    "    if screen_name != '-':\n",
    "        print(f'Scraping information from {screen_name}: {i} of {len(screen_names)}')\n",
    "\n",
    "        try:\n",
    "            # scraping tweets\n",
    "            paginator = tweepy.Paginator(\n",
    "                client.get_users_tweets,                                # The method we want to call \n",
    "                client.get_user(username = screen_name.split('@')[1])[0]['id'],   # Arguments passed to the method - the search query\n",
    "                expansions=['author_id', 'in_reply_to_user_id'], ### get reply from this - add below\n",
    "                tweet_fields=[\"public_metrics\", \"created_at\", 'geo', 'context_annotations', 'referenced_tweets'], \n",
    "                user_fields=['username', 'location'],\n",
    "                max_results=100                                         # Arguments passed to the method - how many tweets per page\n",
    "                #limit=20                                               # Argument passed to the paginator - how many pages to retrieve\n",
    "                )\n",
    "\n",
    "            d_tweet = pd.DataFrame()\n",
    "\n",
    "            for tweet in paginator:\n",
    "                data = tweet.data\n",
    "                df_meta = pd.DataFrame(data)\n",
    "\n",
    "                for i in range(len(df_meta)):\n",
    "                    if df_meta.text[i][0:2] == 'RT':\n",
    "                        rt = df_meta.text[i].split(':',1)\n",
    "                        rt[1] = api.get_status(id=df_meta.id[i], tweet_mode = 'extended')._json['retweeted_status']['full_text']\n",
    "                        df_meta.text[i] = ': '.join(rt)\n",
    "\n",
    "                df_public_metrics = pd.DataFrame()\n",
    "                    # extracting more public metrics (likes, retweets, etc.), which is stored in dictionaries\n",
    "                for public_metric in df_meta.public_metrics:\n",
    "                        # storing the public metrics\n",
    "                    df_public_metrics = pd.concat([df_public_metrics, pd.DataFrame([public_metric])] ,ignore_index=True)\n",
    "                    # collecting the text and the public metrics\n",
    "                df_meta = pd.concat([df_meta.text, df_meta.id, df_meta.created_at, df_public_metrics], axis=1)\n",
    "                    # saving the creator of the tweets\n",
    "                df_meta = df_meta.assign(account = 'BlokAnders')\n",
    "                    # storing the information from each paginator\n",
    "                d_tweet = pd.concat([d_tweet, df_meta], ignore_index=True)\n",
    "                # storing the information from each account\n",
    "            d_tweets = pd.concat([d_tweets, d_tweet], ignore_index=True)\n",
    "        except:\n",
    "            restricted_tweets.append(screen_name)\n",
    "\n",
    "# sleeps after "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#d_tweets.to_csv('d_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n",
      "/var/folders/mb/0425zkts7695hq57ytn4rk7m0000gn/T/ipykernel_59539/2001545292.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_meta.text[i] = ': '.join(rt)\n"
     ]
    }
   ],
   "source": [
    "d_tweets = pd.DataFrame()\n",
    "\n",
    "paginator = tweepy.Paginator(\n",
    "    client.get_users_tweets,                                # The method we want to call \n",
    "    client.get_user(username = 'BlokAnders')[0]['id'],   # Arguments passed to the method - the search query\n",
    "    expansions=['author_id', 'in_reply_to_user_id'], ### get reply from this - add below\n",
    "    tweet_fields=[\"public_metrics\", \"created_at\", 'geo', 'context_annotations', 'referenced_tweets'], \n",
    "    user_fields=['username', 'location'],\n",
    "    max_results=100                                         # Arguments passed to the method - how many tweets per page\n",
    "    #limit=20                                               # Argument passed to the paginator - how many pages to retrieve\n",
    "    )\n",
    "\n",
    "d_tweet = pd.DataFrame()\n",
    "\n",
    "for tweet in paginator:\n",
    "    data = tweet.data\n",
    "    df_meta = pd.DataFrame(data)\n",
    "\n",
    "    for i in range(len(df_meta)):\n",
    "        if df_meta.text[i][0:2] == 'RT':\n",
    "            rt = df_meta.text[i].split(':',1)\n",
    "            rt[1] = api.get_status(id=df_meta.id[i], tweet_mode = 'extended')._json['retweeted_status']['full_text']\n",
    "            df_meta.text[i] = ': '.join(rt)\n",
    "\n",
    "\n",
    "    df_public_metrics = pd.DataFrame()\n",
    "        # extracting more public metrics (likes, retweets, etc.), which is stored in dictionaries\n",
    "    for public_metric in df_meta.public_metrics:\n",
    "            # storing the public metrics\n",
    "        df_public_metrics = pd.concat([df_public_metrics, pd.DataFrame([public_metric])] ,ignore_index=True)\n",
    "        # collecting the text and the public metrics\n",
    "    df_meta = pd.concat([df_meta.text, df_meta.id, df_meta.created_at, df_public_metrics], axis=1)\n",
    "        # saving the creator of the tweets\n",
    "    df_meta = df_meta.assign(account = 'BlokAnders')\n",
    "        # storing the information from each paginator\n",
    "    d_tweet = pd.concat([d_tweet, df_meta], ignore_index=True)\n",
    "    # storing the information from each account\n",
    "d_tweets = pd.concat([d_tweets, d_tweet], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# old version\n",
    "\n",
    "#if exists('d_followers.csv'):\n",
    "#    d_followers = pd.read_csv('d_followers.csv', index_col=0)\n",
    "#else:\n",
    "#    d_followers = pd.DataFrame({'account': [], 'followers' : [], 'locations' : []})\n",
    "#    d_followers.to_csv('d_followers.csv')\n",
    "#\n",
    "#restricted_followers = []\n",
    "#\n",
    "#for i, screen_name in enumerate(screen_names):\n",
    "#    if screen_name != '-':\n",
    "#        if screen_name not in list(d_followers.account):\n",
    "#            print(f'Scraping information from {screen_name.split(\"@\")[1]}: {i} of {len(screen_names)}')\n",
    "#\n",
    "#            try:\n",
    "#                ids = []\n",
    "#\n",
    "#                for fid in tweepy.Cursor(api.get_followers, screen_name=screen_name.split(\"@\")[1], count=100).pages():\n",
    "#                    ids.append(fid)\n",
    "#\n",
    "#                l_followers = []\n",
    "#                l_locations = []\n",
    "#\n",
    "#                for id in ids:\n",
    "#                    for i in id:\n",
    "#                        user_info = jsonify_tweepy(i)\n",
    "#                        l_followers.append(user_info['screen_name'])\n",
    "#                        l_locations.append(user_info['location'])\n",
    "#                account = [screen_name] * len(l_followers)\n",
    "#\n",
    "#                d_temp = pd.DataFrame({'account': account, 'followers' : l_followers, 'locations' : l_locations})\n",
    "#\n",
    "#                d_followers = pd.concat([d_followers, d_temp],axis=0)\n",
    "#\n",
    "#                d_followers.to_csv('d_followers.csv')\n",
    "#                print('saved')\n",
    "#                #time.sleep(60*2.5)\n",
    "#\n",
    "#            except:\n",
    "#                restricted_followers.append(screen_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
